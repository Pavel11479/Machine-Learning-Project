{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns # for visualisation\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rep = 'Downloads/sml-ht2021/'\n",
    "y_train = pd.read_csv(rep + 'y_train.csv', index_col = 0, squeeze=True)\n",
    "X_train = pd.read_csv(rep + 'X_train.csv', index_col = 0, header=[0, 1, 2])\n",
    "X_test = pd.read_csv(rep + 'X_test.csv', index_col = 0, header=[0, 1, 2])\n",
    "from sklearn.model_selection import train_test_split\n",
    "inputs_train, inputs_test, targets_train, targets_test = train_test_split(X_train, y_train, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train_nn = inputs_train.reset_index(drop=True)\n",
    "inputs_test_nn = inputs_test.reset_index(drop=True)\n",
    "targets_train_nn=targets_train.reset_index(drop=True)\n",
    "targets_test_nn=targets_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest (not used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y_pred_RF = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    RF = RandomForestClassifier(n_estimators = 150, max_depth = 50, class_weight = 'balanced')\n",
    "    RF_prob = RF.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    y_pred_RF = np.append(y_pred_RF, RF_prob, axis = 0)\n",
    "\n",
    "\n",
    "dfRF_train = pd.DataFrame(y_pred_RF, columns = ['RF_El', 'RF_Exp', 'RF_Flk', 'RF_HH', \n",
    "                                           'RF_Inst', 'RF_Intr', 'RF_Pop', 'RF_Rck'])\n",
    "\n",
    "RF_fitted = RF.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_RF = RF_fitted.predict_proba(inputs_test)\n",
    "dfRF_test = pd.DataFrame(y_pred_test_RF, columns = ['RF_El', 'RF_Exp', 'RF_Flk', 'RF_HH', \n",
    "                                           'RF_Inst', 'RF_Intr', 'RF_Pop', 'RF_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# PASHA PLEASE CHECK TO MAKE SURE THIS IS THE SAME ORDER OF CLASSES AS YOU ARE USING\n",
    "classes = ['Electronic', 'Experimental', 'Folk', 'Hip-Hop', 'Instrumental','International', 'Pop', 'Rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        super(AudioDataset, self).__init__()\n",
    "        assert x.shape[0] == y.shape[0] # assuming shape[0] = dataset size\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x_tensor = torch.tensor(self.x.loc[index].values, dtype=torch.float)\n",
    "        y_tensor = torch.tensor(self.y.loc[index], dtype=torch.long)\n",
    "        return (x_tensor, y_tensor)\n",
    "    \n",
    "def nn_accuracy(network, data_loader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i,d in enumerate(data_loader):\n",
    "            data, labels = d\n",
    "            outputs = network(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            #print(i,predicted,labels)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "\n",
    "def nn_train(net, train_dl, val_dl, SAVE_PATH, epoch_num=20, num_batches=100, \n",
    "             lr=0.01, momentum=0.9, verbose=False):\n",
    "    # define the optimization\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "    # enumerate epochs\n",
    "    max_val_acc = 0\n",
    "    for epoch in range(epoch_num):\n",
    "        # enumerate mini batches\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, labels) in enumerate(train_dl):\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # training loss\n",
    "            running_loss += loss.item()\n",
    "            val_acc = nn_accuracy(net, val_dl)\n",
    "            if val_acc > max_val_acc:\n",
    "                    torch.save(net.state_dict(), SAVE_PATH)\n",
    "                    max_val_acc = val_acc\n",
    "            if i % num_batches == (num_batches-1): \n",
    "                tr_acc = nn_accuracy(net, train_dl)\n",
    "                val_acc = nn_accuracy(net, val_dl)\n",
    "                if verbose:\n",
    "                    print('[EPOCH #', epoch+1,']') \n",
    "                    print('       Loss:', round(running_loss / num_batches, 3))\n",
    "                    print('  Train Acc:', round(tr_acc,2), '%')\n",
    "                    print('    Val Acc:', round(val_acc,2),'%')\n",
    "                    print('Max Val Acc:', round(max_val_acc,2) ,'%')\n",
    "                    print(\"------------\")\n",
    "                running_loss = 0.0\n",
    "                \n",
    "\n",
    "    print('Finished Training')\n",
    "    print('Max Validation Acc:', max_val_acc, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(X_train.columns), 800)\n",
    "        self.fc2 = nn.Linear(800, 100)\n",
    "        self.fc3 = nn.Linear(100, 500)\n",
    "        self.fc4 = nn.Linear(500, 100)\n",
    "        self.fc5 = nn.Linear(100, 100)\n",
    "        self.fc6 = nn.Linear(100, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.celu(self.fc1(x))\n",
    "        x = torch.celu(self.fc2(x),alpha=2)\n",
    "        x = torch.celu(self.fc3(x),alpha=3)\n",
    "        x = torch.celu(self.fc4(x),alpha=2)\n",
    "        x = torch.celu(self.fc5(x))\n",
    "        x = self.fc6(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_train_to_prob_mat(X_train, y_train, X_test, epoch_num = 20, class_labels=classes, verbose=False):\n",
    "    \n",
    "    y_train = y_train.replace(classes,range(0,len(class_labels)))\n",
    "    \n",
    "    # normalize data\n",
    "    std_scale = StandardScaler()\n",
    "    X_train = std_scale.fit_transform(X_train)\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_test = std_scale.transform(X_test)\n",
    "    X_test = pd.DataFrame(X_test)\n",
    "    \n",
    "    # make dataset\n",
    "    dataset = AudioDataset(X_train,y_train)\n",
    "    \n",
    "    # split into train and validation set for nn training\n",
    "    num_train = int(len(X_train)*.8)\n",
    "    num_val = len(X_train)-num_train\n",
    "    batch_size = 100\n",
    "    num_batches = num_train//batch_size\n",
    "    train, val = random_split(dataset, [num_train,num_val])\n",
    "    \n",
    "    # make dataloaders\n",
    "    train_dl = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    val_dl = DataLoader(val, batch_size=num_val, shuffle=False)\n",
    "    #test_dl = DataLoader(val, batch_size=1, shuffle=False)\n",
    "    \n",
    "    # make folder to save network params if does not exist\n",
    "    if not os.path.exists('./NN_CV_params'):\n",
    "        os.makedirs('./NN_CV_params')\n",
    "    \n",
    "    # train the network\n",
    "    if verbose:\n",
    "        print(\"--- BEGIN TRAINING NETWORK ---\")\n",
    "    net = Net()\n",
    "    nn_train(net, train_dl, val_dl, './NN_CV_params/network_params.pth',epoch_num=epoch_num,\n",
    "             num_batches=num_batches,verbose=verbose)\n",
    "    if verbose:\n",
    "        print(\"--- FINISHED TRAINING NETWORK ---\")\n",
    "    \n",
    "    # generate probability matrix for test data\n",
    "    prob_matrix = []\n",
    "    with torch.no_grad():\n",
    "        for index in range(len(X_test)):\n",
    "            data = torch.tensor(X_test.loc[index].values, dtype=torch.float)\n",
    "            net.load_state_dict(torch.load('./NN_CV_params/network_params.pth'))\n",
    "            outputs = torch.softmax(net(data).unsqueeze(1).T, dim=1)\n",
    "            prob_dist = outputs.tolist()[0]\n",
    "            prob_matrix.append(prob_dist)\n",
    "            \n",
    "    return np.array(prob_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.055\n",
      "  Train Acc: 35.49 %\n",
      "    Val Acc: 34.23 %\n",
      "Max Val Acc: 34.23 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.86\n",
      "  Train Acc: 40.18 %\n",
      "    Val Acc: 37.5 %\n",
      "Max Val Acc: 38.24 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.627\n",
      "  Train Acc: 46.65 %\n",
      "    Val Acc: 41.67 %\n",
      "Max Val Acc: 41.67 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.445\n",
      "  Train Acc: 50.89 %\n",
      "    Val Acc: 48.07 %\n",
      "Max Val Acc: 48.07 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.32\n",
      "  Train Acc: 56.77 %\n",
      "    Val Acc: 49.7 %\n",
      "Max Val Acc: 50.3 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.226\n",
      "  Train Acc: 59.93 %\n",
      "    Val Acc: 51.79 %\n",
      "Max Val Acc: 52.38 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.137\n",
      "  Train Acc: 63.39 %\n",
      "    Val Acc: 51.34 %\n",
      "Max Val Acc: 52.38 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.052\n",
      "  Train Acc: 67.67 %\n",
      "    Val Acc: 50.6 %\n",
      "Max Val Acc: 53.27 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 0.985\n",
      "  Train Acc: 68.42 %\n",
      "    Val Acc: 52.08 %\n",
      "Max Val Acc: 54.32 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.942\n",
      "  Train Acc: 70.68 %\n",
      "    Val Acc: 53.12 %\n",
      "Max Val Acc: 54.32 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.883\n",
      "  Train Acc: 73.18 %\n",
      "    Val Acc: 51.49 %\n",
      "Max Val Acc: 54.32 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.828\n",
      "  Train Acc: 75.26 %\n",
      "    Val Acc: 54.61 %\n",
      "Max Val Acc: 54.61 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.774\n",
      "  Train Acc: 76.82 %\n",
      "    Val Acc: 54.17 %\n",
      "Max Val Acc: 54.61 %\n",
      "------------\n",
      "[EPOCH # 14 ]\n",
      "       Loss: 0.748\n",
      "  Train Acc: 78.79 %\n",
      "    Val Acc: 52.38 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.719\n",
      "  Train Acc: 79.39 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.67\n",
      "  Train Acc: 80.32 %\n",
      "    Val Acc: 52.53 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.636\n",
      "  Train Acc: 80.54 %\n",
      "    Val Acc: 52.23 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.594\n",
      "  Train Acc: 83.85 %\n",
      "    Val Acc: 51.93 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.542\n",
      "  Train Acc: 86.16 %\n",
      "    Val Acc: 51.34 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.505\n",
      "  Train Acc: 88.36 %\n",
      "    Val Acc: 52.68 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 55.05952380952381 %\n",
      "--- FINISHED TRAINING NETWORK ---\n",
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.059\n",
      "  Train Acc: 29.95 %\n",
      "    Val Acc: 28.87 %\n",
      "Max Val Acc: 28.87 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.878\n",
      "  Train Acc: 36.31 %\n",
      "    Val Acc: 34.52 %\n",
      "Max Val Acc: 34.52 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.689\n",
      "  Train Acc: 42.67 %\n",
      "    Val Acc: 40.18 %\n",
      "Max Val Acc: 40.18 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.512\n",
      "  Train Acc: 49.37 %\n",
      "    Val Acc: 45.24 %\n",
      "Max Val Acc: 45.24 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.361\n",
      "  Train Acc: 55.32 %\n",
      "    Val Acc: 46.13 %\n",
      "Max Val Acc: 47.62 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.258\n",
      "  Train Acc: 59.34 %\n",
      "    Val Acc: 49.11 %\n",
      "Max Val Acc: 49.4 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.158\n",
      "  Train Acc: 63.91 %\n",
      "    Val Acc: 52.38 %\n",
      "Max Val Acc: 53.12 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.074\n",
      "  Train Acc: 65.96 %\n",
      "    Val Acc: 53.72 %\n",
      "Max Val Acc: 53.72 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 1.0\n",
      "  Train Acc: 68.38 %\n",
      "    Val Acc: 53.27 %\n",
      "Max Val Acc: 54.46 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.928\n",
      "  Train Acc: 72.17 %\n",
      "    Val Acc: 54.91 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.873\n",
      "  Train Acc: 74.44 %\n",
      "    Val Acc: 53.42 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.848\n",
      "  Train Acc: 74.07 %\n",
      "    Val Acc: 54.91 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.782\n",
      "  Train Acc: 78.31 %\n",
      "    Val Acc: 53.42 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 14 ]\n",
      "       Loss: 0.742\n",
      "  Train Acc: 79.5 %\n",
      "    Val Acc: 52.98 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.696\n",
      "  Train Acc: 80.58 %\n",
      "    Val Acc: 52.23 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.639\n",
      "  Train Acc: 83.04 %\n",
      "    Val Acc: 52.53 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.612\n",
      "  Train Acc: 82.48 %\n",
      "    Val Acc: 49.4 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.564\n",
      "  Train Acc: 84.23 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.542\n",
      "  Train Acc: 85.68 %\n",
      "    Val Acc: 50.89 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.495\n",
      "  Train Acc: 84.71 %\n",
      "    Val Acc: 50.3 %\n",
      "Max Val Acc: 55.8 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 55.80357142857143 %\n",
      "--- FINISHED TRAINING NETWORK ---\n",
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.058\n",
      "  Train Acc: 30.88 %\n",
      "    Val Acc: 29.17 %\n",
      "Max Val Acc: 29.17 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.884\n",
      "  Train Acc: 39.03 %\n",
      "    Val Acc: 35.42 %\n",
      "Max Val Acc: 35.86 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.657\n",
      "  Train Acc: 44.49 %\n",
      "    Val Acc: 42.86 %\n",
      "Max Val Acc: 42.86 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.468\n",
      "  Train Acc: 51.08 %\n",
      "    Val Acc: 47.92 %\n",
      "Max Val Acc: 47.92 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.334\n",
      "  Train Acc: 56.44 %\n",
      "    Val Acc: 50.0 %\n",
      "Max Val Acc: 51.19 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.224\n",
      "  Train Acc: 60.12 %\n",
      "    Val Acc: 51.93 %\n",
      "Max Val Acc: 51.93 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.139\n",
      "  Train Acc: 62.65 %\n",
      "    Val Acc: 51.64 %\n",
      "Max Val Acc: 52.53 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.063\n",
      "  Train Acc: 65.44 %\n",
      "    Val Acc: 50.3 %\n",
      "Max Val Acc: 52.83 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 1.011\n",
      "  Train Acc: 69.87 %\n",
      "    Val Acc: 51.34 %\n",
      "Max Val Acc: 53.42 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.948\n",
      "  Train Acc: 70.39 %\n",
      "    Val Acc: 50.74 %\n",
      "Max Val Acc: 53.42 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.883\n",
      "  Train Acc: 72.54 %\n",
      "    Val Acc: 51.19 %\n",
      "Max Val Acc: 53.42 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.835\n",
      "  Train Acc: 75.52 %\n",
      "    Val Acc: 51.19 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.783\n",
      "  Train Acc: 76.86 %\n",
      "    Val Acc: 52.23 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 14 ]\n",
      "       Loss: 0.747\n",
      "  Train Acc: 77.6 %\n",
      "    Val Acc: 53.57 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.702\n",
      "  Train Acc: 79.65 %\n",
      "    Val Acc: 52.53 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.66\n",
      "  Train Acc: 80.32 %\n",
      "    Val Acc: 51.04 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.638\n",
      "  Train Acc: 84.0 %\n",
      "    Val Acc: 52.08 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.584\n",
      "  Train Acc: 84.45 %\n",
      "    Val Acc: 49.4 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.557\n",
      "  Train Acc: 83.93 %\n",
      "    Val Acc: 49.26 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.489\n",
      "  Train Acc: 86.42 %\n",
      "    Val Acc: 52.38 %\n",
      "Max Val Acc: 54.91 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 54.910714285714285 %\n",
      "--- FINISHED TRAINING NETWORK ---\n",
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.045\n",
      "  Train Acc: 32.81 %\n",
      "    Val Acc: 29.02 %\n",
      "Max Val Acc: 29.61 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.836\n",
      "  Train Acc: 37.65 %\n",
      "    Val Acc: 36.9 %\n",
      "Max Val Acc: 37.35 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.697\n",
      "  Train Acc: 41.0 %\n",
      "    Val Acc: 36.76 %\n",
      "Max Val Acc: 37.35 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.533\n",
      "  Train Acc: 48.29 %\n",
      "    Val Acc: 45.09 %\n",
      "Max Val Acc: 45.09 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.391\n",
      "  Train Acc: 52.19 %\n",
      "    Val Acc: 47.77 %\n",
      "Max Val Acc: 48.51 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.278\n",
      "  Train Acc: 57.59 %\n",
      "    Val Acc: 52.38 %\n",
      "Max Val Acc: 52.38 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.18\n",
      "  Train Acc: 61.76 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 53.72 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.104\n",
      "  Train Acc: 65.07 %\n",
      "    Val Acc: 54.91 %\n",
      "Max Val Acc: 55.36 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 1.048\n",
      "  Train Acc: 68.01 %\n",
      "    Val Acc: 54.32 %\n",
      "Max Val Acc: 55.65 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.966\n",
      "  Train Acc: 71.28 %\n",
      "    Val Acc: 55.21 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.903\n",
      "  Train Acc: 72.28 %\n",
      "    Val Acc: 53.72 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.844\n",
      "  Train Acc: 74.59 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.815\n",
      "  Train Acc: 76.19 %\n",
      "    Val Acc: 53.57 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EPOCH # 14 ]\n",
      "       Loss: 0.765\n",
      "  Train Acc: 77.49 %\n",
      "    Val Acc: 53.12 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.699\n",
      "  Train Acc: 80.21 %\n",
      "    Val Acc: 54.32 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.674\n",
      "  Train Acc: 80.69 %\n",
      "    Val Acc: 51.93 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.618\n",
      "  Train Acc: 82.89 %\n",
      "    Val Acc: 53.57 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.587\n",
      "  Train Acc: 84.0 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.571\n",
      "  Train Acc: 85.19 %\n",
      "    Val Acc: 50.3 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.538\n",
      "  Train Acc: 86.57 %\n",
      "    Val Acc: 52.08 %\n",
      "Max Val Acc: 57.14 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 57.142857142857146 %\n",
      "--- FINISHED TRAINING NETWORK ---\n",
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.051\n",
      "  Train Acc: 28.24 %\n",
      "    Val Acc: 28.27 %\n",
      "Max Val Acc: 28.27 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.873\n",
      "  Train Acc: 36.38 %\n",
      "    Val Acc: 37.8 %\n",
      "Max Val Acc: 37.8 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.678\n",
      "  Train Acc: 43.97 %\n",
      "    Val Acc: 43.6 %\n",
      "Max Val Acc: 43.6 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.501\n",
      "  Train Acc: 49.96 %\n",
      "    Val Acc: 47.47 %\n",
      "Max Val Acc: 47.47 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.371\n",
      "  Train Acc: 54.39 %\n",
      "    Val Acc: 51.64 %\n",
      "Max Val Acc: 51.93 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.265\n",
      "  Train Acc: 59.26 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 53.87 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.18\n",
      "  Train Acc: 62.95 %\n",
      "    Val Acc: 54.76 %\n",
      "Max Val Acc: 55.06 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.099\n",
      "  Train Acc: 65.33 %\n",
      "    Val Acc: 54.32 %\n",
      "Max Val Acc: 55.36 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 1.026\n",
      "  Train Acc: 68.15 %\n",
      "    Val Acc: 55.65 %\n",
      "Max Val Acc: 56.7 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.975\n",
      "  Train Acc: 70.42 %\n",
      "    Val Acc: 56.7 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.923\n",
      "  Train Acc: 73.62 %\n",
      "    Val Acc: 57.74 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.853\n",
      "  Train Acc: 75.15 %\n",
      "    Val Acc: 57.89 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.796\n",
      "  Train Acc: 74.74 %\n",
      "    Val Acc: 59.08 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 14 ]\n",
      "       Loss: 0.778\n",
      "  Train Acc: 77.86 %\n",
      "    Val Acc: 56.99 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.721\n",
      "  Train Acc: 79.84 %\n",
      "    Val Acc: 57.14 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.677\n",
      "  Train Acc: 82.29 %\n",
      "    Val Acc: 55.65 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.607\n",
      "  Train Acc: 82.51 %\n",
      "    Val Acc: 53.87 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.572\n",
      "  Train Acc: 84.82 %\n",
      "    Val Acc: 54.46 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.549\n",
      "  Train Acc: 85.04 %\n",
      "    Val Acc: 54.32 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.542\n",
      "  Train Acc: 87.98 %\n",
      "    Val Acc: 57.29 %\n",
      "Max Val Acc: 59.23 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 59.226190476190474 %\n",
      "--- FINISHED TRAINING NETWORK ---\n",
      "--- BEGIN TRAINING NETWORK ---\n",
      "[EPOCH # 1 ]\n",
      "       Loss: 2.057\n",
      "  Train Acc: 33.18 %\n",
      "    Val Acc: 32.89 %\n",
      "Max Val Acc: 33.18 %\n",
      "------------\n",
      "[EPOCH # 2 ]\n",
      "       Loss: 1.89\n",
      "  Train Acc: 36.27 %\n",
      "    Val Acc: 35.57 %\n",
      "Max Val Acc: 35.57 %\n",
      "------------\n",
      "[EPOCH # 3 ]\n",
      "       Loss: 1.691\n",
      "  Train Acc: 43.45 %\n",
      "    Val Acc: 38.39 %\n",
      "Max Val Acc: 38.39 %\n",
      "------------\n",
      "[EPOCH # 4 ]\n",
      "       Loss: 1.51\n",
      "  Train Acc: 49.59 %\n",
      "    Val Acc: 46.88 %\n",
      "Max Val Acc: 46.88 %\n",
      "------------\n",
      "[EPOCH # 5 ]\n",
      "       Loss: 1.372\n",
      "  Train Acc: 54.43 %\n",
      "    Val Acc: 50.15 %\n",
      "Max Val Acc: 50.6 %\n",
      "------------\n",
      "[EPOCH # 6 ]\n",
      "       Loss: 1.277\n",
      "  Train Acc: 58.37 %\n",
      "    Val Acc: 50.6 %\n",
      "Max Val Acc: 51.64 %\n",
      "------------\n",
      "[EPOCH # 7 ]\n",
      "       Loss: 1.183\n",
      "  Train Acc: 61.27 %\n",
      "    Val Acc: 52.23 %\n",
      "Max Val Acc: 52.98 %\n",
      "------------\n",
      "[EPOCH # 8 ]\n",
      "       Loss: 1.112\n",
      "  Train Acc: 65.36 %\n",
      "    Val Acc: 54.32 %\n",
      "Max Val Acc: 54.61 %\n",
      "------------\n",
      "[EPOCH # 9 ]\n",
      "       Loss: 1.026\n",
      "  Train Acc: 68.23 %\n",
      "    Val Acc: 54.17 %\n",
      "Max Val Acc: 55.21 %\n",
      "------------\n",
      "[EPOCH # 10 ]\n",
      "       Loss: 0.984\n",
      "  Train Acc: 69.31 %\n",
      "    Val Acc: 53.27 %\n",
      "Max Val Acc: 55.21 %\n",
      "------------\n",
      "[EPOCH # 11 ]\n",
      "       Loss: 0.929\n",
      "  Train Acc: 70.46 %\n",
      "    Val Acc: 53.12 %\n",
      "Max Val Acc: 55.21 %\n",
      "------------\n",
      "[EPOCH # 12 ]\n",
      "       Loss: 0.874\n",
      "  Train Acc: 72.51 %\n",
      "    Val Acc: 54.61 %\n",
      "Max Val Acc: 55.65 %\n",
      "------------\n",
      "[EPOCH # 13 ]\n",
      "       Loss: 0.817\n",
      "  Train Acc: 75.97 %\n",
      "    Val Acc: 52.83 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 14 ]\n",
      "       Loss: 0.777\n",
      "  Train Acc: 74.18 %\n",
      "    Val Acc: 50.45 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 15 ]\n",
      "       Loss: 0.767\n",
      "  Train Acc: 78.27 %\n",
      "    Val Acc: 51.64 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 16 ]\n",
      "       Loss: 0.695\n",
      "  Train Acc: 80.62 %\n",
      "    Val Acc: 52.98 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 17 ]\n",
      "       Loss: 0.642\n",
      "  Train Acc: 79.65 %\n",
      "    Val Acc: 52.98 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 18 ]\n",
      "       Loss: 0.601\n",
      "  Train Acc: 84.52 %\n",
      "    Val Acc: 52.08 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 19 ]\n",
      "       Loss: 0.552\n",
      "  Train Acc: 83.97 %\n",
      "    Val Acc: 54.17 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "[EPOCH # 20 ]\n",
      "       Loss: 0.52\n",
      "  Train Acc: 86.31 %\n",
      "    Val Acc: 53.87 %\n",
      "Max Val Acc: 55.95 %\n",
      "------------\n",
      "Finished Training\n",
      "Max Validation Acc: 55.95238095238095 %\n",
      "--- FINISHED TRAINING NETWORK ---\n"
     ]
    }
   ],
   "source": [
    "y_pred_NN = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    \n",
    "    inputs_train_d = inputs_train.drop(inputs_train.index[i-840:i])\n",
    "    targets_train_d = targets_train.drop(targets_train.index[i-840:i])\n",
    "    inputs_train_td = inputs_train[i-840:i]\n",
    "    \n",
    "    inputs_train_nn = inputs_train_d.reset_index(drop=True)\n",
    "    inputs_test_nn = inputs_train_td.reset_index(drop=True)\n",
    "    targets_train_nn=targets_train_d.reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    NN_prob = nn_train_to_prob_mat(inputs_train_nn,\n",
    "                                    targets_train_nn, \n",
    "                                    inputs_test_nn, class_labels=classes, epoch_num=20, verbose=True)\n",
    "    y_pred_NN = np.append(y_pred_NN, NN_prob, axis = 0)\n",
    "\n",
    "dfNN_train = pd.DataFrame(y_pred_NN, columns = ['NN_El', 'NN_Exp', 'NN_Flk', 'NN_HH', \n",
    "                                           'NN_Inst', 'NN_Intr', 'NN_Pop', 'NN_Rck'])\n",
    "\n",
    "inputs_train_nnf = inputs_train.reset_index(drop=True)\n",
    "targets_train_nnf = targets_train.reset_index(drop=True)\n",
    "inputs_test_nnf = inputs_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "y_pred_test_NN = nn_train_to_prob_mat(inputs_train_nnf,\n",
    "                                    targets_train_nnf, inputs_test_nnf, class_labels=classes, epoch_num=20, verbose=True)\n",
    "\n",
    "dfNN_test = pd.DataFrame(y_pred_test_NN, columns = ['NN_El', 'NN_Exp', 'NN_Flk', 'NN_HH', \n",
    "                                           'NN_Inst', 'NN_Intr', 'NN_Pop', 'NN_Rck'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:52:43] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:52:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:53:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:53:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:54:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:54:17] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:55:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:55:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:55:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:55:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:56:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:56:40] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#XGBoost1\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "y_pred_XGB = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    XGB = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "     n_estimators=320,\n",
    "     max_depth=7,\n",
    "     min_child_weight=5,\n",
    "     gamma=0,\n",
    "     subsample=0.6,\n",
    "     colsample_bytree=0.5,\n",
    "     reg_alpha = 0.1,\n",
    "     reg_lambda = 1,\n",
    "     objective= 'multi:softmax',\n",
    "     num_class = 8,\n",
    "     n_jobs=-1,\n",
    "     scale_pos_weight=1,\n",
    "     seed=27)\n",
    "    XGB_prob = XGB.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    y_pred_XGB = np.append(y_pred_XGB, XGB_prob, axis = 0)\n",
    "    \n",
    "\n",
    "dfXGB_train = pd.DataFrame(y_pred_XGB, columns = ['XGB1_El', 'XGB1_Exp', 'XGB1_Flk', 'XGB1_HH', \n",
    "                                           'XGB1_Inst', 'XGB1_Intr', 'XGB1_Pop', 'XGB1_Rck'])\n",
    "\n",
    "XGB_fitted = XGB.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_XGB = XGB_fitted.predict_proba(inputs_test)\n",
    "dfXGB_test = pd.DataFrame(y_pred_test_XGB, columns = ['XGB1_El', 'XGB1_Exp', 'XGB1_Flk', 'XGB1_HH', \n",
    "                                           'XGB1_Inst', 'XGB1_Intr', 'XGB1_Pop', 'XGB1_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:57:48] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:57:48] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:58:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:58:29] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:59:11] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[13:59:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[13:59:52] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:00:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:00:33] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:01:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:01:14] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#XGBoost2\n",
    "\n",
    "y_pred_XGB2 = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    XGB2 = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "        n_estimators=320,\n",
    "        max_depth=7,\n",
    "        min_child_weight=5,\n",
    "        gamma=0.2,\n",
    "        subsample=0.6,\n",
    "        colsample_bytree=0.4,\n",
    "        reg_alpha = 0.1,\n",
    "        reg_lambda = 1,\n",
    "        objective= 'multi:softmax',\n",
    "        num_class = 8,\n",
    "        n_jobs=-1,\n",
    "        scale_pos_weight=1,\n",
    "        seed=27)\n",
    "    XGB2_prob = XGB2.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    y_pred_XGB2 = np.append(y_pred_XGB2, XGB2_prob, axis = 0)\n",
    "    \n",
    "dfXGB2_train = pd.DataFrame(y_pred_XGB2, columns = ['XGB2_El', 'XGB2_Exp', 'XGB2_Flk', 'XGB2_HH', \n",
    "                                           'XGB2_Inst', 'XGB2_Intr', 'XGB2_Pop', 'XGB2_Rck'])\n",
    "\n",
    "XGB2_fitted = XGB2.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_XGB2 = XGB2_fitted.predict_proba(inputs_test)\n",
    "dfXGB2_test = pd.DataFrame(y_pred_test_XGB2, columns = ['XGB2_El', 'XGB2_Exp', 'XGB2_Flk', 'XGB2_HH', \n",
    "                                           'XGB2_Inst', 'XGB2_Intr', 'XGB2_Pop', 'XGB2_Rck'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:02:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:02:09] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:02:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:02:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:03:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:03:18] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:03:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:03:55] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:04:30] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:04:31] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[14:05:04] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:05:05] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "#XGBoost3\n",
    "\n",
    "y_pred_XGB3 = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    XGB3 = XGBClassifier(\n",
    "     learning_rate =0.1,\n",
    "      n_estimators=320,\n",
    "      max_depth=3,\n",
    "      min_child_weight=5,\n",
    "      gamma=0,\n",
    "      subsample=0.6,\n",
    "      colsample_bytree=0.5,\n",
    "      reg_alpha = 1,\n",
    "      reg_lambda = 1,\n",
    "      objective= 'multi:softmax',\n",
    "      num_class = 8,\n",
    "      n_jobs=-1,\n",
    "      scale_pos_weight=1,\n",
    "      seed=27)\n",
    "\n",
    "    XGB3_prob = XGB3.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    \n",
    "    y_pred_XGB3 = np.append(y_pred_XGB3, XGB3_prob, axis = 0)\n",
    "\n",
    "dfXGB3_train = pd.DataFrame(y_pred_XGB3, columns = ['XGB3_El', 'XGB3_Exp', 'XGB3_Flk', 'XGB3_HH', \n",
    "                                           'XGB3_Inst', 'XGB3_Intr', 'XGB3_Pop', 'XGB3_Rck'])\n",
    "\n",
    "XGB3_fitted = XGB3.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_XGB3 = XGB3_fitted.predict_proba(inputs_test)\n",
    "dfXGB3_test = pd.DataFrame(y_pred_test_XGB3, columns = ['XGB3_El', 'XGB3_Exp', 'XGB3_Flk', 'XGB3_HH', \n",
    "                                           'XGB3_Inst', 'XGB3_Intr', 'XGB3_Pop', 'XGB3_Rck'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LDA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "y_pred_LDA = np.empty(shape = (0,8))\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    LDA = LinearDiscriminantAnalysis(solver = \"eigen\", shrinkage = 'auto')\n",
    "\n",
    "    LDA_prob = LDA.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    \n",
    "    y_pred_LDA = np.append(y_pred_LDA, LDA_prob, axis = 0)\n",
    "    \n",
    "dfLDA_train = pd.DataFrame(y_pred_LDA, columns = ['LDA_El', 'LDA_Exp', 'LDA_Flk', 'LDA_HH', \n",
    "                                           'LDA_Inst', 'LDA_Intr', 'LDA_Pop', 'LDA_Rck'])\n",
    "\n",
    "LDA_fitted = LDA.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_LDA = LDA_fitted.predict_proba(inputs_test)\n",
    "dfLDA_test = pd.DataFrame(y_pred_test_LDA, columns = ['LDA_El', 'LDA_Exp', 'LDA_Flk', 'LDA_HH', \n",
    "                                           'LDA_Inst', 'LDA_Intr', 'LDA_Pop', 'LDA_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QDA\n",
    "\n",
    "#PCA data preparation\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "inputs_train_sc = scaler.transform(inputs_train)\n",
    "inputs_test_sc = scaler.transform(inputs_test)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 200)\n",
    "pca.fit(X_train_sc)\n",
    "inputs_train_pca = pca.transform(inputs_train_sc)\n",
    "inputs_test_pca = pca.transform(inputs_test_sc)\n",
    "\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "y_pred_QDA = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    QDA = QuadraticDiscriminantAnalysis()\n",
    "    QDA_prob = QDA.fit(np.delete(inputs_train_pca, slice(i-840, i), axis = 0), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train_pca[i-840:i])\n",
    "\n",
    "    y_pred_QDA = np.append(y_pred_QDA, QDA_prob, axis = 0)\n",
    "    \n",
    "dfQDA_train = pd.DataFrame(y_pred_QDA, columns = ['QDA_El', 'QDA_Exp', 'QDA_Flk', 'QDA_HH', \n",
    "                                           'QDA_Inst', 'QDA_Intr', 'QDA_Pop', 'QDA_Rck'])\n",
    "\n",
    "QDA_fitted = QDA.fit(inputs_train_pca, targets_train)\n",
    "\n",
    "y_pred_test_QDA = QDA_fitted.predict_proba(inputs_test_pca)\n",
    "dfQDA_test = pd.DataFrame(y_pred_test_QDA, columns = ['QDA_El', 'QDA_Exp', 'QDA_Flk', 'QDA_HH', \n",
    "                                           'QDA_Inst', 'QDA_Intr', 'QDA_Pop', 'QDA_Rck'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbours "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "y_pred_KNN = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "\n",
    "    KNN = KNeighborsClassifier(n_neighbors=20)\n",
    "\n",
    "    KNN_prob = KNN.fit(np.delete(inputs_train_sc, slice(i-840, i), axis = 0), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train_sc[i-840:i])\n",
    "    \n",
    "    y_pred_KNN = np.append(y_pred_KNN, KNN_prob, axis = 0)\n",
    "    \n",
    "dfKNN_train = pd.DataFrame(y_pred_KNN, columns = ['KNN_El', 'KNN_Exp', 'KNN_Flk', 'KNN_HH', \n",
    "                                           'KNN_Inst', 'KNN_Intr', 'KNN_Pop', 'KNN_Rck'])\n",
    "\n",
    "KNN_fitted = KNN.fit(inputs_train_sc, targets_train)\n",
    "\n",
    "y_pred_test_KNN = KNN_fitted.predict_proba(inputs_test_sc)\n",
    "dfKNN_test = pd.DataFrame(y_pred_test_KNN, columns = ['KNN_El', 'KNN_Exp', 'KNN_Flk', 'KNN_HH', \n",
    "                                           'KNN_Inst', 'KNN_Intr', 'KNN_Pop', 'KNN_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "y_pred_LR = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    LR = LogisticRegression(max_iter = 50)\n",
    "    \n",
    "    LR_prob = LR.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    \n",
    "    y_pred_LR = np.append(y_pred_LR, LR_prob, axis = 0)\n",
    "    \n",
    "dfLR_train = pd.DataFrame(y_pred_LR, columns = ['LR_El', 'LR_Exp', 'LR_Flk', 'LR_HH', \n",
    "                                           'LR_Inst', 'LR_Intr', 'LR_Pop', 'LR_Rck'])\n",
    "\n",
    "LR_fitted = LR.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_LR = LR_fitted.predict_proba(inputs_test)\n",
    "dfLR_test = pd.DataFrame(y_pred_test_LR, columns = ['LR_El', 'LR_Exp', 'LR_Flk', 'LR_HH', \n",
    "                                           'LR_Inst', 'LR_Intr', 'LR_Pop', 'LR_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "y_pred_NB = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "    NB = GaussianNB()\n",
    "    \n",
    "    NB_prob = NB.fit(inputs_train.drop(inputs_train.index[i-840:i]), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train[i-840:i])\n",
    "    \n",
    "    y_pred_NB = np.append(y_pred_NB, NB_prob, axis = 0)\n",
    "    \n",
    "    \n",
    "dfNB_train = pd.DataFrame(y_pred_NB, columns = ['NB_El', 'NB_Exp', 'NB_Flk', 'NB_HH', \n",
    "                                           'NB_Inst', 'NB_Intr', 'NB_Pop', 'NB_Rck'])\n",
    "\n",
    "NB_fitted = NB.fit(inputs_train, targets_train)\n",
    "\n",
    "y_pred_test_NB = NB_fitted.predict_proba(inputs_test)\n",
    "\n",
    "dfNB_test = pd.DataFrame(y_pred_test_NB, columns = ['NB_El', 'NB_Exp', 'NB_Flk', 'NB_HH', \n",
    "                                           'NB_Inst', 'NB_Intr', 'NB_Pop', 'NB_Rck'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:368: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    }
   ],
   "source": [
    "#Extratrees \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "indices = pd.read_csv('Downloads/RF_indices.csv', index_col = 0, squeeze=True).to_numpy()\n",
    "i = 190\n",
    "ind = indices[0:i :]\n",
    "inputs_train_rf = inputs_train_sc[:,ind]\n",
    "inputs_test_rf = inputs_test_sc[:,ind]\n",
    "\n",
    "\n",
    "y_pred_XTR = np.empty(shape = (0,8))\n",
    "\n",
    "for i in [840, 1680, 2520, 3360, 4200]:\n",
    "\n",
    "    XTR = ExtraTreesClassifier(warm_start = True, n_estimators = 840, max_features = None, min_samples_split = 2, \n",
    "                          random_state = 102)\n",
    "    \n",
    "    XTR_prob = XTR.fit(np.delete(inputs_train_rf, slice(i-840, i), axis = 0), \n",
    "                       targets_train.drop(targets_train.index[i-840:i])).predict_proba(inputs_train_rf[i-840:i])\n",
    "    \n",
    "    y_pred_XTR = np.append(y_pred_XTR, XTR_prob, axis = 0)\n",
    "\n",
    "dfXTR_train = pd.DataFrame(y_pred_XTR, columns = ['XTR_El', 'XTR_Exp', 'XTR_Flk', 'XTR_HH', \n",
    "                                           'XTR_Inst', 'XTR_Intr', 'XTR_Pop', 'XTR_Rck'])\n",
    "\n",
    "XTR_fitted = XTR.fit(inputs_train_rf, targets_train)\n",
    "\n",
    "y_pred_test_XTR = XTR_fitted.predict_proba(inputs_test_rf)\n",
    "\n",
    "dfXTR_test = pd.DataFrame(y_pred_test_XTR, columns = ['XTR_El', 'XTR_Exp', 'XTR_Flk', 'XTR_HH', \n",
    "                                           'XTR_Inst', 'XTR_Intr', 'XTR_Pop', 'XTR_Rck'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability matrix combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_train_t = pd.concat([dfXGB_train, dfLDA_train, dfQDA_train, \n",
    "                              dfKNN_train, dfLR_train, dfXGB2_train, dfXGB3_train,  dfNB_train, dfXTR_train, dfNN_train],\n",
    "                               axis = 1 )\n",
    "pred_proba_test_t = pd.concat([dfXGB_test, dfLDA_test, dfQDA_test,\n",
    "                            dfKNN_test, dfLR_test, dfXGB2_test, dfXGB3_test, dfNB_test, dfXTR_test, dfNN_test], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top-level classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy IS:  0.6302380952380953\n",
      "Prediction accuracy OS:  0.6394444444444445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter = 150)\n",
    "log_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', log_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', log_reg.score(pred_proba_test_t, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy IS:  0.6204761904761905\n",
      "Prediction accuracy OS:  0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "LDA_reg = LinearDiscriminantAnalysis(solver = \"eigen\", shrinkage = 'auto')\n",
    "LDA_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', LDA_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', LDA_reg.score(pred_proba_test_t, targets_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:33:44] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prediction accuracy IS:  0.9997619047619047\n",
      "Prediction accuracy OS:  0.63\n"
     ]
    }
   ],
   "source": [
    "XGB_reg = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    " n_estimators=320,\n",
    " max_depth=7,\n",
    " min_child_weight=5,\n",
    " gamma=0,\n",
    " subsample=0.6,\n",
    " colsample_bytree=0.5,\n",
    " reg_alpha = 0.1,\n",
    " reg_lambda = 1,\n",
    " objective= 'multi:softmax',\n",
    " num_class = 8,\n",
    " n_jobs=-1,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "\n",
    "XGB_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', XGB_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', XGB_reg.score(pred_proba_test_t, targets_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy IS:  0.5919047619047619\n",
      "Prediction accuracy OS:  0.6016666666666667\n"
     ]
    }
   ],
   "source": [
    "NB_reg = GaussianNB()\n",
    "NB_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', NB_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', NB_reg.score(pred_proba_test_t, targets_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy IS:  0.7861904761904762\n",
      "Prediction accuracy OS:  0.63\n"
     ]
    }
   ],
   "source": [
    "RF_reg = RandomForestClassifier(n_estimators = 50, max_depth = 10, class_weight = 'balanced')\n",
    "RF_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', RF_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', RF_reg.score(pred_proba_test_t, targets_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:33:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:33:58] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy IS:  0.9997619047619047\n",
      "Prediction accuracy OS:  0.6233333333333333\n"
     ]
    }
   ],
   "source": [
    "XGB2_reg = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    "    n_estimators=320,\n",
    "    max_depth=7,\n",
    "    min_child_weight=5,\n",
    "    gamma=0.2,\n",
    "    subsample=0.6,\n",
    "    colsample_bytree=0.4,\n",
    "    reg_alpha = 0.1,\n",
    "    reg_lambda = 1,\n",
    "    objective= 'multi:softmax',\n",
    "    num_class = 8,\n",
    "    n_jobs=-1,\n",
    "    scale_pos_weight=1,\n",
    "    seed=27)\n",
    "XGB2_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', XGB2_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', XGB2_reg.score(pred_proba_test_t, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:34:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:541: \n",
      "Parameters: { scale_pos_weight } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:34:08] WARNING: /Users/travis/build/dmlc/xgboost/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Prediction accuracy IS:  0.9252380952380952\n",
      "Prediction accuracy OS:  0.6327777777777778\n"
     ]
    }
   ],
   "source": [
    "XGB3_reg = XGBClassifier(\n",
    " learning_rate =0.1,\n",
    "  n_estimators=320,\n",
    "  max_depth=3,\n",
    "  min_child_weight=5,\n",
    "  gamma=0,\n",
    "  subsample=0.6,\n",
    "  colsample_bytree=0.5,\n",
    "  reg_alpha = 1,\n",
    "  reg_lambda = 1,\n",
    "  objective= 'multi:softmax',\n",
    "  num_class = 8,\n",
    "  n_jobs=-1,\n",
    "  scale_pos_weight=1,\n",
    "  seed=27)\n",
    "XGB3_reg.fit(pred_proba_train_t, targets_train)\n",
    "print('Prediction accuracy IS: ', XGB3_reg.score(pred_proba_train_t, targets_train))\n",
    "print('Prediction accuracy OS: ', XGB3_reg.score(pred_proba_test_t, targets_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x3000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(30, 30), dpi = 100)\n",
    "plot_confusion_matrix(estimator = XGB_reg, X = pred_proba_test_t, y_true = targets_test, xticks_rotation = 'vertical')\n",
    "plt.savefig('Documents/SML_prac_fig/confusion_mat_STC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_train_t.to_csv(r'Documents/debug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
